#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡ä¸‹è½½æ¨¡å—
è´Ÿè´£å¤„ç†å›¾ç‰‡çš„ä¸‹è½½ã€é‡è¯•å’Œæ–‡ä»¶ç®¡ç†
"""

import os
import time
import requests
from urllib.parse import urlparse
from typing import List, Optional, Tuple
from bs4 import BeautifulSoup

from config import REQUEST_TIMEOUT, IMAGE_TIMEOUT, CSS_SELECTORS
from utils import normalize_url


class ImageDownloader:
    """å›¾ç‰‡ä¸‹è½½å™¨ç±»"""
    
    def __init__(self, session: requests.Session):
        """
        åˆå§‹åŒ–å›¾ç‰‡ä¸‹è½½å™¨
        
        Args:
            session: ç”¨äºä¸‹è½½çš„requestsä¼šè¯
        """
        self.session = session
    
    def download_images(self, image_links: List[str], referer_url: str, output_path: str) -> Tuple[List[str], bool]:
        """
        ä¸‹è½½å›¾ç‰‡åˆ—è¡¨
        
        Args:
            image_links: å›¾ç‰‡é“¾æ¥åˆ—è¡¨
            referer_url: å¼•ç”¨é¡µé¢URL
            output_path: è¾“å‡ºç›®å½•è·¯å¾„
            
        Returns:
            tuple: (downloaded_files: List[str], success: bool)
                - downloaded_files: æˆåŠŸä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                - success: æ˜¯å¦è‡³å°‘æˆåŠŸä¸‹è½½äº†ä¸€å¼ å›¾ç‰‡
        """
        print("å¼€å§‹ä¸‹è½½å›¾ç‰‡...")
        downloaded_files = []
        
        for i, img_url in enumerate(image_links):
            print(f"ä¸‹è½½å›¾ç‰‡ {i+1}/{len(image_links)}: {img_url[:50]}...")
            file_path = self.download_single_image(img_url, referer_url, output_path)
            
            if file_path:
                downloaded_files.append(file_path)
            else:
                # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°è·å–é¡µé¢è·å–æ–°çš„å›¾ç‰‡URL
                print(f"  ğŸ”„ å°è¯•é‡æ–°è·å–é¡µé¢ä»¥è·å–æ–°çš„å›¾ç‰‡URL...")
                new_image_links = self._get_fresh_image_links(referer_url)
                if new_image_links and i < len(new_image_links):
                    new_img_url = new_image_links[i]
                    file_path = self.download_single_image(new_img_url, referer_url, output_path)
                    if file_path:
                        downloaded_files.append(file_path)
        
        success = len(downloaded_files) > 0
        print(f"æˆåŠŸä¸‹è½½ {len(downloaded_files)} å¼ å›¾ç‰‡")
        
        return downloaded_files, success
    
    def download_single_image(self, image_url: str, referer_url: str, output_path: str) -> Optional[str]:
        """
        ä¸‹è½½å•ä¸ªå›¾ç‰‡
        
        Args:
            image_url: å›¾ç‰‡URL
            referer_url: å¼•ç”¨é¡µé¢URL
            output_path: è¾“å‡ºç›®å½•è·¯å¾„
            
        Returns:
            str: æˆåŠŸä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_path, exist_ok=True)
            
            # è®¾ç½®å›¾ç‰‡ä¸‹è½½è¯·æ±‚å¤´
            headers = {
                'Referer': referer_url,
                'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'image',
                'Sec-Fetch-Mode': 'no-cors',
                'Sec-Fetch-Site': 'cross-site',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
            }
            
            # å°è¯•ä¸‹è½½å›¾ç‰‡ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•é‡æ–°è·å–é¡µé¢
            max_retries = 2
            for attempt in range(max_retries):
                # ä½¿ç”¨å½“å‰sessionå‘é€è¯·æ±‚
                response = self.session.get(image_url, headers=headers, timeout=IMAGE_TIMEOUT)
                response.raise_for_status()
                
                # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '')
                if not content_type.startswith('image/'):
                    print(f"  âœ— å“åº”ä¸æ˜¯å›¾ç‰‡æ ¼å¼: {content_type}")
                    if attempt < max_retries - 1:
                        print(f"  âš ï¸ å°è¯•é‡æ–°è·å–é¡µé¢...")
                        # é‡æ–°è®¿é—®äº§å“é¡µé¢ä»¥è·å–æ–°çš„ç­¾åURL
                        self._refresh_image_urls(referer_url)
                        continue
                    return None
                
                # ç”Ÿæˆæ–‡ä»¶å
                parsed_url = urlparse(image_url)
                filename = os.path.basename(parsed_url.path)
                if not filename or '.' not in filename:
                    # ä»content-typeè·å–æ‰©å±•å
                    ext = 'jpg'
                    if 'png' in content_type:
                        ext = 'png'
                    elif 'webp' in content_type:
                        ext = 'webp'
                    filename = f"image_{int(time.time())}.{ext}"
                
                # ä¿å­˜æ–‡ä»¶
                file_path = os.path.join(output_path, filename)
                with open(file_path, 'wb') as f:
                    f.write(response.content)
                
                print(f"  âœ“ å›¾ç‰‡å·²ä¿å­˜: {file_path}")
                return file_path

        except Exception as e:
            print(f"  âœ— å›¾ç‰‡ä¸‹è½½å¤±è´¥: {str(e)[:100]}...")
            return None
    
    def _get_fresh_image_links(self, product_url: str) -> List[str]:
        """
        é‡æ–°è·å–äº§å“é¡µé¢çš„å›¾ç‰‡é“¾æ¥ä»¥è§£å†³CloudFrontç­¾åè¿‡æœŸé—®é¢˜
        
        Args:
            product_url: äº§å“é¡µé¢URL
            
        Returns:
            List[str]: æ–°çš„å›¾ç‰‡é“¾æ¥åˆ—è¡¨
        """
        try:
            print(f"  ğŸ”„ é‡æ–°è®¿é—®äº§å“é¡µé¢: {product_url}")
            response = self.session.get(product_url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # é‡æ–°è§£æé¡µé¢è·å–æ–°çš„å›¾ç‰‡é“¾æ¥
            soup = BeautifulSoup(response.text, 'html.parser')
            thumbnail_wrapper = soup.find(class_=CSS_SELECTORS['thumbnail_wrapper'])
            
            if thumbnail_wrapper:
                new_image_links = []
                img_elements = thumbnail_wrapper.find_all('img')
                for img in img_elements:
                    src = img.get('src')
                    if src:
                        src = normalize_url(src)
                        new_image_links.append(src)
                
                print(f"  âœ“ è·å–åˆ° {len(new_image_links)} ä¸ªæ–°çš„å›¾ç‰‡é“¾æ¥")
                return new_image_links
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡å®¹å™¨")
                return []
                
        except Exception as e:
            print(f"  âš ï¸ é‡æ–°è·å–å›¾ç‰‡é“¾æ¥å¤±è´¥: {str(e)[:50]}...")
            return []
    
    def _refresh_image_urls(self, product_url: str):
        """
        é‡æ–°è®¿é—®äº§å“é¡µé¢ä»¥è·å–æ–°çš„CloudFrontç­¾åURL
        
        Args:
            product_url: äº§å“é¡µé¢URL
        """
        try:
            print(f"  ğŸ”„ é‡æ–°è®¿é—®äº§å“é¡µé¢ä»¥è·å–æ–°çš„å›¾ç‰‡URL...")
            response = self.session.get(product_url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # é‡æ–°è§£æé¡µé¢è·å–æ–°çš„å›¾ç‰‡é“¾æ¥
            soup = BeautifulSoup(response.text, 'html.parser')
            thumbnail_wrapper = soup.find(class_=CSS_SELECTORS['thumbnail_wrapper'])
            
            if thumbnail_wrapper:
                # æ›´æ–°å›¾ç‰‡é“¾æ¥ç¼“å­˜ï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºç¼“å­˜æœºåˆ¶ï¼‰
                print(f"  âœ“ æˆåŠŸåˆ·æ–°å›¾ç‰‡URL")
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°æ–°çš„å›¾ç‰‡å®¹å™¨")
                
        except Exception as e:
            print(f"  âš ï¸ åˆ·æ–°å›¾ç‰‡URLå¤±è´¥: {str(e)[:50]}...")
